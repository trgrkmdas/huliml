# Hyperparameter Tuning Mimari TasarÄ±mÄ±

## ğŸ¯ Genel BakÄ±ÅŸ

Modern, modÃ¼ler ve yÃ¼ksek performanslÄ± hyperparameter tuning sistemi.

## ğŸ“ KlasÃ¶r YapÄ±sÄ±

```
src/model_training/
â”œâ”€â”€ tuning/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # BaseTuner abstract class
â”‚   â”œâ”€â”€ grid_search.py       # GridSearchCV wrapper
â”‚   â”œâ”€â”€ randomized_search.py # RandomizedSearchCV wrapper
â”‚   â”œâ”€â”€ optuna_tuner.py      # Optuna wrapper (opsiyonel)
â”‚   â””â”€â”€ tuner.py             # HyperparameterTuner ana sÄ±nÄ±fÄ±
```

## ğŸ—ï¸ Mimari TasarÄ±m

### 1. BaseTuner (Abstract Base Class)

**Dosya**: `src/model_training/tuning/base.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class BaseTuner(ABC):
    """Base tuner interface"""
    
    @abstractmethod
    def tune(self, model, X, y, param_grid, **kwargs):
        """Perform hyperparameter tuning"""
        pass
    
    @abstractmethod
    def get_best_params(self) -> Dict[str, Any]:
        """Get best parameters"""
        pass
    
    @abstractmethod
    def get_best_score(self) -> float:
        """Get best score"""
        pass
```

### 2. RandomizedSearchTuner (Ã–ncelikli - HÄ±zlÄ±)

**Dosya**: `src/model_training/tuning/randomized_search.py`

```python
from sklearn.model_selection import RandomizedSearchCV
from .base import BaseTuner

class RandomizedSearchTuner(BaseTuner):
    """RandomizedSearchCV wrapper - HÄ±zlÄ± ve etkili"""
    
    def __init__(self, n_iter=50, cv=5, n_jobs=-1, scoring='accuracy'):
        # RandomizedSearchCV ile tuning
        # Parallel processing desteÄŸi
        # Caching desteÄŸi
```

### 3. GridSearchTuner

**Dosya**: `src/model_training/tuning/grid_search.py`

```python
from sklearn.model_selection import GridSearchCV
from .base import BaseTuner

class GridSearchTuner(BaseTuner):
    """GridSearchCV wrapper - Exhaustive search"""
    
    def __init__(self, cv=5, n_jobs=-1, scoring='accuracy'):
        # GridSearchCV ile tuning
        # KÃ¼Ã§Ã¼k parametre space'leri iÃ§in
```

### 4. OptunaTuner (Opsiyonel - En Ä°yi)

**Dosya**: `src/model_training/tuning/optuna_tuner.py`

```python
import optuna
from .base import BaseTuner

class OptunaTuner(BaseTuner):
    """Optuna wrapper - Bayesian optimization"""
    
    def __init__(self, n_trials=100, timeout=None):
        # Optuna ile tuning
        # En geliÅŸmiÅŸ ve hÄ±zlÄ±
        # Ek paket gerektirir: optuna
```

### 5. HyperparameterTuner (Ana SÄ±nÄ±f)

**Dosya**: `src/model_training/tuning/tuner.py`

```python
class HyperparameterTuner:
    """Ana hyperparameter tuning sÄ±nÄ±fÄ±"""
    
    def __init__(self, config=None):
        self.config = config or get_config()
        self.tuner = None  # BaseTuner instance
        self.best_params = None
        self.best_score = None
        self.tuning_history = []
    
    def tune(self, model, X, y, param_grid, **kwargs):
        """Hyperparameter tuning yap"""
        # Tuning metodunu seÃ§ (config'den)
        # Tuning yap
        # Best parametreleri kaydet
        # History kaydet
    
    def get_best_params(self) -> Dict[str, Any]:
        """Best parametreleri dÃ¶ndÃ¼r"""
    
    def incremental_tune(self, model, X, y, coarse_grid, fine_grid):
        """Ä°ki aÅŸamalÄ± tuning (coarse â†’ fine)"""
        # 1. Coarse search (geniÅŸ aralÄ±klar)
        # 2. Fine search (dar aralÄ±klar, best Ã§evresinde)
```

## âš¡ Performans OptimizasyonlarÄ±

### 1. Parallel Processing

```python
# n_jobs=-1: TÃ¼m CPU core'larÄ± kullan
tuner = RandomizedSearchTuner(n_jobs=-1)
```

### 2. Caching MekanizmasÄ±

```python
class TuningCache:
    """Tuning sonuÃ§larÄ±nÄ± cache'le"""
    
    def __init__(self):
        self.cache = {}  # {param_hash: score}
    
    def get(self, params):
        """Cache'den al"""
        param_hash = self._hash_params(params)
        return self.cache.get(param_hash)
    
    def set(self, params, score):
        """Cache'e kaydet"""
        param_hash = self._hash_params(params)
        self.cache[param_hash] = score
```

### 3. Early Stopping Entegrasyonu

```python
# Her tuning denemesinde early stopping kullan
# Gereksiz iterasyonlarÄ± Ã¶nle
model.fit(X, y, early_stopping_rounds=50, ...)
```

### 4. Incremental Tuning

```python
# Ä°ki aÅŸamalÄ± tuning
# 1. Coarse: GeniÅŸ aralÄ±klar, az iterasyon
coarse_grid = {
    'num_leaves': [15, 31, 63, 127],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
}

# 2. Fine: Dar aralÄ±klar, Ã§ok iterasyon (best Ã§evresinde)
fine_grid = {
    'num_leaves': [best_num_leaves-10, best_num_leaves, best_num_leaves+10],
    'learning_rate': [best_lr-0.01, best_lr, best_lr+0.01],
}
```

### 5. Time-Based Pruning (Optuna)

```python
# Zaman limiti ile tuning
study.optimize(objective, timeout=3600)  # 1 saat
```

## ğŸ”„ Walk-Forward Validation Entegrasyonu

### YaklaÅŸÄ±m 1: TÃ¼m Fold'lar iÃ§in Ortak Tuning (HÄ±zlÄ±)

```python
# Ã–nce tÃ¼m fold'lar iÃ§in ortak tuning
best_params = tuner.tune(model, X_train, y_train, param_grid)

# Sonra best parametrelerle walk-forward validation
trainer.walk_forward_validation()
```

### YaklaÅŸÄ±m 2: Her Fold iÃ§in AyrÄ± Tuning (Daha DoÄŸru)

```python
# Her fold iÃ§in ayrÄ± tuning (yavaÅŸ ama daha doÄŸru)
for fold in walk_forward_splits:
    best_params_fold = tuner.tune(model, X_train_fold, y_train_fold, param_grid)
```

### Ã–nerilen: Hibrit YaklaÅŸÄ±m

```python
# 1. Coarse tuning (tÃ¼m fold'lar iÃ§in ortak, hÄ±zlÄ±)
coarse_best = tuner.tune(model, X_train, y_train, coarse_grid)

# 2. Fine tuning (best Ã§evresinde)
fine_best = tuner.incremental_tune(model, X_train, y_train, coarse_best, fine_grid)

# 3. Walk-forward validation (best parametrelerle)
trainer.walk_forward_validation()
```

## âš™ï¸ Config Entegrasyonu

```python
@dataclass
class TuningConfig:
    """Hyperparameter tuning konfigÃ¼rasyonu"""
    
    # Tuning ayarlarÄ±
    enable_tuning: bool = False
    tuning_method: str = "randomized"  # 'grid', 'randomized', 'optuna'
    
    # RandomizedSearchCV parametreleri
    randomized_n_iter: int = 50  # Deneme sayÄ±sÄ±
    randomized_cv: int = 5  # Cross-validation fold sayÄ±sÄ±
    
    # GridSearchCV parametreleri
    grid_cv: int = 5
    
    # Optuna parametreleri
    optuna_n_trials: int = 100
    optuna_timeout: Optional[int] = None  # Saniye cinsinden
    
    # Performans
    n_jobs: int = -1  # Parallel processing (-1 = tÃ¼m core'lar)
    use_cache: bool = True  # Caching aktif mi?
    
    # Incremental tuning
    use_incremental: bool = True  # Ä°ki aÅŸamalÄ± tuning
    coarse_n_iter: int = 20  # Coarse aÅŸama iterasyon sayÄ±sÄ±
    fine_n_iter: int = 50  # Fine aÅŸama iterasyon sayÄ±sÄ±
    
    # Parametre grid'leri (LightGBM iÃ§in Ã¶rnek)
    param_grids: Optional[Dict[str, List[Any]]] = None
    
    def __post_init__(self):
        if self.param_grids is None:
            # LightGBM iÃ§in varsayÄ±lan grid
            self.param_grids = {
                'num_leaves': [15, 31, 63, 127],
                'learning_rate': [0.01, 0.05, 0.1],
                'feature_fraction': [0.8, 0.9, 1.0],
                'bagging_fraction': [0.7, 0.8, 0.9],
                'min_child_samples': [10, 20, 30],
            }
```

## ğŸš€ KullanÄ±m SenaryolarÄ±

### Senaryo 1: Basit Tuning (RandomizedSearch)

```python
from src.model_training import ModelTrainer
from src.model_training.tuning import HyperparameterTuner

trainer = ModelTrainer()
trainer.prepare_data(df_features)
trainer.split_data(X, y)
trainer.apply_preprocessing()

# Tuning
tuner = HyperparameterTuner()
best_params = tuner.tune(
    trainer.model,
    trainer.X_train_scaled,
    trainer.y_train,
    param_grid=config.model.tuning.param_grids
)

# Best parametrelerle model eÄŸit
trainer.model.set_params(**best_params)
trainer.train_model()
```

### Senaryo 2: Incremental Tuning

```python
# Coarse tuning
coarse_grid = {
    'num_leaves': [15, 31, 63, 127],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
}

coarse_best = tuner.tune(model, X, y, coarse_grid, n_iter=20)

# Fine tuning (best Ã§evresinde)
fine_grid = {
    'num_leaves': [coarse_best['num_leaves']-10, 
                   coarse_best['num_leaves'], 
                   coarse_best['num_leaves']+10],
    'learning_rate': [coarse_best['learning_rate']-0.01,
                     coarse_best['learning_rate'],
                     coarse_best['learning_rate']+0.01],
}

fine_best = tuner.tune(model, X, y, fine_grid, n_iter=50)
```

### Senaryo 3: Walk-Forward ile Tuning

```python
# 1. Tuning (tÃ¼m train set Ã¼zerinde)
best_params = tuner.tune(model, X_train, y_train, param_grid)

# 2. Best parametrelerle walk-forward validation
trainer.model.set_params(**best_params)
results = trainer.walk_forward_validation()
```

## ğŸ“Š Performans Metrikleri

```python
tuning_results = {
    'best_params': {...},
    'best_score': 0.85,
    'tuning_time': 120.5,  # saniye
    'n_trials': 50,
    'improvement': 0.05,  # Baseline'a gÃ¶re iyileÅŸme
    'tuning_history': [...],  # Her deneme iÃ§in skor
}
```

## âœ… SonuÃ§

**Mimari Ã–zellikleri:**
- âœ… ModÃ¼ler (BaseTuner, concrete implementasyonlar)
- âœ… YÃ¼ksek performanslÄ± (parallel, caching, incremental)
- âœ… Mevcut sistemle entegre (ModelTrainer, Config)
- âœ… Walk-forward validation uyumlu
- âœ… FarklÄ± tuning stratejileri (Grid, Randomized, Optuna)

**Performans OptimizasyonlarÄ±:**
- âœ… Parallel processing (n_jobs=-1)
- âœ… Caching mekanizmasÄ±
- âœ… Early stopping entegrasyonu
- âœ… Incremental tuning
- âœ… Time-based pruning (Optuna)

